{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1vnZ2rNDzojPoyCZ11Icq5LWq3ovjEVB6","authorship_tag":"ABX9TyNjlH8oj7cnrjcRFifWA4Tl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!nvidia-smi\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLWVKZrfsEAd","executionInfo":{"status":"ok","timestamp":1733903278390,"user_tz":-540,"elapsed":469,"user":{"displayName":"구준모","userId":"09347892568623850386"}},"outputId":"24c7af3d-8b66-4ba8-e950-f5193e726f0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Dec 11 07:47:57 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0              48W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import os\n","new_path = '/content/drive/MyDrive/linear_rep_geometry-main'\n","os.chdir(new_path)\n","print(\"Changed Working Directory to:\", os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O90YsiclrWTK","executionInfo":{"status":"ok","timestamp":1733916577233,"user_tz":-540,"elapsed":884,"user":{"displayName":"구준모","userId":"09347892568623850386"}},"outputId":"919b2b64-81ab-448b-b9a1-4279116e246e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Changed Working Directory to: /content/drive/MyDrive/linear_rep_geometry-main\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybVrv_Y1kg3E"},"outputs":[],"source":["import os\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# 1. Counterfactual Pairs 처리\n","def get_counterfactual_pairs(filename, tokenizer, max_pairs=10):\n","    with open(filename, 'r') as f:\n","        lines = f.readlines()\n","\n","    word_pairs = [line.strip().split('\\t') for line in lines if line.strip()][:max_pairs]\n","    base_words = [pair[0] for pair in word_pairs]\n","    target_words = [pair[1] for pair in word_pairs]\n","\n","    # Tokenize and get token indices\n","    base_tokens = [tokenizer.encode(word, add_special_tokens=False) for word in base_words]\n","    target_tokens = [tokenizer.encode(word, add_special_tokens=False) for word in target_words]\n","\n","    return base_tokens, target_tokens\n","\n","# 2. Embedding 계산 및 Conceptual Vector 생성\n","def compute_conceptual_vectors(base_tokens, target_tokens, model, tokenizer, device=\"cuda\"):\n","    base_sentences = [tokenizer.decode(tokens) for tokens in base_tokens]\n","    target_sentences = [tokenizer.decode(tokens) for tokens in target_tokens]\n","\n","    sentences = base_sentences + target_sentences\n","    embeddings = get_embeddings(sentences, model, tokenizer, device)\n","\n","    base_embeddings = embeddings[:len(base_tokens)]\n","    target_embeddings = embeddings[len(base_tokens):]\n","\n","    diff_embeddings = target_embeddings - base_embeddings\n","    mean_vector = torch.mean(diff_embeddings, dim=0)\n","    return mean_vector\n","\n","def get_embeddings(sentences, model, tokenizer, device=\"cuda\", max_length=512):\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    # max_length와 truncation 추가\n","    inputs = tokenizer(\n","        sentences,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=max_length\n","    ).to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","        hidden_states = outputs.hidden_states[-1]\n","\n","    attention_mask = inputs[\"attention_mask\"]\n","    expanded_mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n","    summed_embeddings = torch.sum(hidden_states * expanded_mask, dim=1)\n","    summed_mask = torch.sum(expanded_mask, dim=1)\n","    embeddings = summed_embeddings / summed_mask\n","\n","    return embeddings\n","\n","\n","\n","def compute_projection_matrix(conceptual_vectors, hidden_dim):\n","    \"\"\"\n","    Conceptual Vectors로부터 Projection Matrix를 계산 (QR 기반).\n","\n","    Args:\n","        conceptual_vectors: torch.Tensor, Conceptual Vectors (shape: num_concepts x hidden_dim).\n","        hidden_dim: int, 모델의 hidden dimension (예: 1024).\n","\n","    Returns:\n","        projection_matrix: torch.Tensor, Projection Matrix (shape: hidden_dim x hidden_dim).\n","    \"\"\"\n","    # QR 분해를 사용하여 orthogonal basis 계산\n","    orthogonal_basis = torch.linalg.qr(conceptual_vectors.T)[0]\n","\n","    # Projection Matrix 계산 (hidden_dim 크기로 확장)\n","    projection_matrix = orthogonal_basis @ orthogonal_basis.T\n","    return projection_matrix\n","\n","\n","\n","# 5. Orthogonal Procrustes Analysis (OPA)\n","def orthogonal_procrustes(A, B):\n","    \"\"\"\n","    두 Projection Matrix를 align하기 위한 Orthogonal Procrustes Analysis.\n","    \"\"\"\n","    M = B.T @ A\n","    U, _, V = torch.linalg.svd(M)\n","    R = U @ V.T\n","    return R\n","\n","def run_experiment_for_all_files(directory, model_1, tokenizer_1, model_2, tokenizer_2):\n","    concept_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.txt')]\n","\n","    all_conceptual_vectors_1 = []\n","    all_conceptual_vectors_2 = []\n","\n","    for concept_file in concept_files:\n","        print(f\"\\nProcessing file: {concept_file}\")\n","\n","        # Counterfactual Pairs 로드\n","        base_tokens_1, target_tokens_1 = get_counterfactual_pairs(concept_file, tokenizer_1)\n","        base_tokens_2, target_tokens_2 = get_counterfactual_pairs(concept_file, tokenizer_2)\n","\n","        # Conceptual Vectors 계산\n","        conceptual_vectors_1 = compute_conceptual_vectors(base_tokens_1, target_tokens_1, model_1, tokenizer_1)\n","        conceptual_vectors_2 = compute_conceptual_vectors(base_tokens_2, target_tokens_2, model_2, tokenizer_2)\n","\n","        all_conceptual_vectors_1.append(conceptual_vectors_1)\n","        all_conceptual_vectors_2.append(conceptual_vectors_2)\n","\n","    # Projection Matrix 계산\n","    subspace_1 = torch.stack(all_conceptual_vectors_1)\n","    subspace_2 = torch.stack(all_conceptual_vectors_2)\n","\n","    hidden_dim = subspace_1.shape[1]  # Hidden dimension (e.g., 1024)\n","    P1 = compute_projection_matrix(subspace_1, hidden_dim)\n","    P2 = compute_projection_matrix(subspace_2, hidden_dim)\n","\n","    # Orthogonal Procrustes Analysis로 R 계산\n","    R = orthogonal_procrustes(P1, P2)\n","\n","    # 학습된 R을 model_2에 반영\n","    update_model_with_R(model_2, R)\n","\n","    print(\"\\nExperiment completed. Model 2 embedding layer updated.\")\n"]},{"cell_type":"code","source":["def update_model_with_R(model, R):\n","    \"\"\"\n","    모델의 embedding layer를 정렬 행렬 R을 사용해 업데이트.\n","\n","    Args:\n","        model: Pretrained 모델.\n","        R: torch.Tensor, 학습된 정렬 행렬 (shape: hidden_dim x hidden_dim).\n","    \"\"\"\n","    # 모델의 embedding layer 가져오기\n","    embedding_layer = model.get_input_embeddings()\n","\n","    # 기존 embedding layer의 가중치\n","    embedding_weights = embedding_layer.weight.data\n","\n","    # 정렬 행렬 R을 이용하여 embedding 가중치 변환\n","    transformed_weights = embedding_weights @ R.T\n","\n","    # 업데이트된 embedding 가중치를 embedding layer에 반영\n","    embedding_layer.weight.data = transformed_weights\n","\n","    print(\"Model embedding layer updated with R.\")\n"],"metadata":{"id":"MjbXJXTg5U0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 및 토크나이저 설정\n","model_1_name = \"gpt2-medium\"\n","model_2_name = \"bigscience/bloom-560m\"\n","\n","tokenizer_1 = AutoTokenizer.from_pretrained(model_1_name)\n","model_1 = AutoModelForCausalLM.from_pretrained(model_1_name).to(\"cuda\")\n","\n","tokenizer_2 = AutoTokenizer.from_pretrained(model_2_name)\n","model_2 = AutoModelForCausalLM.from_pretrained(model_2_name).to(\"cuda\")\n","\n","# word_pairs 디렉토리에서 모든 파일 실행\n","directory = \"word_pairs\"\n","run_experiment_for_all_files(directory, model_1, tokenizer_1, model_2, tokenizer_2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iDYtHjsCvylK","executionInfo":{"status":"ok","timestamp":1733906773844,"user_tz":-540,"elapsed":6692,"user":{"displayName":"구준모","userId":"09347892568623850386"}},"outputId":"2fc8eb0d-611e-484b-a1af-4e43958a7396"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Processing file: word_pairs/[adj - comparative].txt\n","\n","Processing file: word_pairs/[Ving - Ved].txt\n","\n","Processing file: word_pairs/[French - German].txt\n","\n","Processing file: word_pairs/[3pSg - Ved].txt\n","\n","Processing file: word_pairs/[adj - superlative].txt\n","\n","Processing file: word_pairs/[adj - adj + ly].txt\n","\n","Processing file: word_pairs/[Ving - 3pSg].txt\n","\n","Processing file: word_pairs/[English - French].txt\n","\n","Processing file: word_pairs/[French - Spanish].txt\n","\n","Processing file: word_pairs/[German - Spanish].txt\n","\n","Processing file: word_pairs/[frequent - infrequent].txt\n","\n","Processing file: word_pairs/[verb - V + ment].txt\n","\n","Processing file: word_pairs/[adj - un + adj].txt\n","\n","Processing file: word_pairs/[small - big].txt\n","\n","Processing file: word_pairs/[thing - part].txt\n","\n","Processing file: word_pairs/[verb - V + er].txt\n","\n","Processing file: word_pairs/[verb - 3pSg].txt\n","\n","Processing file: word_pairs/[verb - V + able].txt\n","\n","Processing file: word_pairs/[verb - Ving].txt\n","\n","Processing file: word_pairs/[verb - V + tion].txt\n","\n","Processing file: word_pairs/[verb - Ved].txt\n","\n","Processing file: word_pairs/[thing - color].txt\n","\n","Processing file: word_pairs/[lower - upper].txt\n","\n","Processing file: word_pairs/[pronoun - possessive].txt\n","\n","Processing file: word_pairs/[noun - plural].txt\n","\n","Processing file: word_pairs/[country - capital].txt\n","\n","Processing file: word_pairs/[male - female].txt\n","Model embedding layer updated with R.\n","\n","Experiment completed. Model 2 embedding layer updated.\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","from sklearn.metrics import mean_squared_error\n","from scipy.stats import pearsonr  # 수정된 부분\n","import numpy as np\n","\n","# 데이터셋 준비 (임의의 문장 쌍)\n","\n","\n","# 레이블: 유사도 점수 (0: 매우 다름, 1: 매우 유사)\n","\n","\n","\n","# 문장 쌍을 모델로부터 임베딩 벡터로 변환하는 함수\n","def compute_sentence_embeddings(sentences, model, tokenizer, device=\"cuda\"):\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    inputs = tokenizer(\n","        sentences,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=512\n","    ).to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","        hidden_states = outputs.hidden_states[-1]\n","\n","    attention_mask = inputs[\"attention_mask\"]\n","    expanded_mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n","    summed_embeddings = torch.sum(hidden_states * expanded_mask, dim=1)\n","    summed_mask = torch.sum(expanded_mask, dim=1)\n","    embeddings = summed_embeddings / summed_mask\n","\n","    return embeddings.cpu()\n","\n","# 코사인 유사도 계산 함수\n","def cosine_similarity(emb1, emb2):\n","    dot_product = np.dot(emb1, emb2)\n","    norm1 = np.linalg.norm(emb1)\n","    norm2 = np.linalg.norm(emb2)\n","    return dot_product / (norm1 * norm2)\n","\n","# 모델 성능 평가 함수\n","def evaluate_model(sentence_pairs, labels, model, tokenizer, device=\"cuda\"):\n","    model = model.to(device)\n","    predicted_similarities = []\n","\n","    for sent1, sent2 in sentence_pairs:\n","        embeddings = compute_sentence_embeddings([sent1, sent2], model, tokenizer, device)\n","        similarity = cosine_similarity(embeddings[0].numpy(), embeddings[1].numpy())\n","        predicted_similarities.append(similarity)\n","\n","    # Pearson Correlation\n","    pearson_corr = pearsonr(labels, predicted_similarities)[0]\n","\n","    # Mean Squared Error\n","    mse = mean_squared_error(labels, predicted_similarities)\n","\n","    return pearson_corr, mse\n","\n","# 모델 비교\n","print(\"Evaluating Original Model 2:\")\n","original_pearson, original_mse = evaluate_model(sentence_pairs, labels, original_model_2, tokenizer_2)\n","\n","print(\"Evaluating Updated Model 2:\")\n","updated_pearson, updated_mse = evaluate_model(sentence_pairs, labels, model_2, tokenizer_2)\n","\n","# 결과 출력\n","print(\"\\nPerformance Comparison:\")\n","print(f\"Original Model 2 - Pearson Correlation: {original_pearson:.4f}, MSE: {original_mse:.4f}\")\n","print(f\"Updated Model 2 - Pearson Correlation: {updated_pearson:.4f}, MSE: {updated_mse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"aaPSU1nDyu62","executionInfo":{"status":"error","timestamp":1733907653168,"user_tz":-540,"elapsed":2255,"user":{"displayName":"구준모","userId":"09347892568623850386"}},"outputId":"eb7aa847-249e-4ab6-ef5e-882f405d885a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating Original Model 2:\n"]},{"output_type":"error","ename":"ValueError","evalue":"x and y must have the same length.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-79d370bd1721>\u001b[0m in \u001b[0;36m<cell line: 167>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# 모델 비교\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating Original Model 2:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0moriginal_pearson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_model_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating Updated Model 2:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-79d370bd1721>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(sentence_pairs, labels, model, tokenizer, device)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Pearson Correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mpearson_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_similarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Mean Squared Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36mpearsonr\u001b[0;34m(x, y, alternative, method)\u001b[0m\n\u001b[1;32m   4719\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4721\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x and y must have the same length.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: x and y must have the same length."]}]},{"cell_type":"code","source":["original_model_2_name = \"bigscience/bloom-560m\"\n","\n","\n","original_tokenizer_2 = AutoTokenizer.from_pretrained(original_model_2_name)\n","original_model_2 = AutoModelForCausalLM.from_pretrained(original_model_2_name).to(\"cuda\")"],"metadata":{"id":"qgCFtMPayglr","executionInfo":{"status":"ok","timestamp":1733926163860,"user_tz":-540,"elapsed":3989,"user":{"displayName":"구준모","userId":"09347892568623850386"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","from scipy.stats import pearsonr\n","from sklearn.metrics import mean_squared_error\n","\n","# **1. Load STS Benchmark Dataset (TSV Format)**\n","def load_sts_data(filepath):\n","    \"\"\"\n","    Load STS Benchmark dataset from a TSV file.\n","\n","    \"\"\"\n","\n","    data = pd.read_csv(filepath, sep='\\t', quoting=3, skiprows=1, header=None)\n","    data.columns = ['index', 'genre', 'filename', 'year','old_index','source1','source2', 'sentence1', 'sentence2','score']\n","\n","    data['score'] = data['score'].astype(float)\n","\n","    return data[['sentence1', 'sentence2', 'score']]\n","\n","\n","# **2. Compute Sentence Embeddings**\n","def compute_sentence_embeddings(sentences, model, tokenizer, device=\"cuda\"):\n","    \"\"\"\n","    Compute embeddings for a list of sentences using the given model and tokenizer.\n","    \"\"\"\n","    tokenizer.pad_token = tokenizer.eos_token\n","    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","        hidden_states = outputs.hidden_states[-1]  # Use the last hidden layer\n","        attention_mask = inputs[\"attention_mask\"]\n","        expanded_mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n","        summed_embeddings = torch.sum(hidden_states * expanded_mask, dim=1)\n","        summed_mask = torch.sum(expanded_mask, dim=1)\n","        embeddings = summed_embeddings / summed_mask\n","    return embeddings\n","\n","# **3. Compute Semantic Similarity**\n","def compute_similarity(sentence_pairs, model, tokenizer, device=\"cuda\"):\n","    \"\"\"\n","    Compute semantic similarity scores for a list of sentence pairs.\n","    \"\"\"\n","    similarities = []\n","    for sentence1, sentence2 in sentence_pairs:\n","        embeddings = compute_sentence_embeddings([sentence1, sentence2], model, tokenizer, device)\n","        sim = torch.cosine_similarity(embeddings[0].unsqueeze(0), embeddings[1].unsqueeze(0)).item()\n","        similarities.append(sim)\n","    return similarities\n","\n","# **4. Evaluate Model**\n","def evaluate_model(data, model, tokenizer, device=\"cuda\"):\n","    \"\"\"\n","    Evaluate the model on STS Benchmark dataset and return Pearson correlation and MSE.\n","    \"\"\"\n","    sentence_pairs = list(zip(data['sentence1'], data['sentence2']))\n","    true_scores = data['score'].tolist()\n","\n","    # Normalize true scores to [0, 1] for consistency\n","    true_scores = [score / 5.0 for score in true_scores]\n","\n","    # Compute model's similarity predictions\n","    predicted_scores = compute_similarity(sentence_pairs, model, tokenizer, device)\n","\n","    # Compute evaluation metrics\n","    pearson_corr = pearsonr(predicted_scores, true_scores)[0]\n","    mse = mean_squared_error(true_scores, predicted_scores)\n","    return pearson_corr, mse\n","\n","# **5. Main Experiment**\n","def run_sts_experiment(data_filepath, original_model, updated_model, tokenizer, device=\"cuda\"):\n","    \"\"\"\n","    Run the STS Benchmark experiment comparing the original and updated models.\n","    \"\"\"\n","    # Load STS data\n","    data = load_sts_data(data_filepath)\n","\n","    print(\"Evaluating Original Model...\")\n","    original_corr, original_mse = evaluate_model(data, original_model, tokenizer, device)\n","    print(f\"Original Model - Pearson Correlation: {original_corr:.4f}, MSE: {original_mse:.4f}\")\n","\n","    print(\"\\nEvaluating Updated Model...\")\n","    updated_corr, updated_mse = evaluate_model(data, updated_model, tokenizer, device)\n","    print(f\"Updated Model - Pearson Correlation: {updated_corr:.4f}, MSE: {updated_mse:.4f}\")\n","\n","    return {\n","        \"original\": {\"pearson\": original_corr, \"mse\": original_mse},\n","        \"updated\": {\"pearson\": updated_corr, \"mse\": updated_mse},\n","    }\n","\n","# **6. Example Usage**\n","if __name__ == \"__main__\":\n","    # Paths and Model Loading\n","    sts_filepath = \"train.tsv\"  # Update this to the actual path\n","    original_model = original_model_2\n","    updated_model = model_2  # Replace with your updated model after applying R\n","\n","    # Run the experiment\n","    results = run_sts_experiment(sts_filepath, original_model, updated_model, tokenizer_2)\n","    print(\"Results:\", results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJBhpuxH66-7","executionInfo":{"status":"ok","timestamp":1733926419869,"user_tz":-540,"elapsed":254361,"user":{"displayName":"구준모","userId":"09347892568623850386"}},"outputId":"66e1df5e-3cc7-4647-b101-870da835acfc"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"output_type":"stream","name":"stdout","text":["Evaluating Original Model...\n","Original Model - Pearson Correlation: 0.0483, MSE: 0.2939\n","\n","Evaluating Updated Model...\n","Updated Model - Pearson Correlation: 0.1261, MSE: 0.2759\n","Results: {'original': {'pearson': 0.048264908377855, 'mse': 0.29387649791896014}, 'updated': {'pearson': 0.12607134881281734, 'mse': 0.27594532095588475}}\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# 1. Counterfactual Pairs 처리\n","def get_counterfactual_pairs(filename, tokenizer, max_pairs=10):\n","    with open(filename, 'r') as f:\n","        lines = f.readlines()\n","\n","    word_pairs = [line.strip().split('\\t') for line in lines if line.strip()][:max_pairs]\n","    base_words = [pair[0] for pair in word_pairs]\n","    target_words = [pair[1] for pair in word_pairs]\n","\n","    # Tokenize and get token indices\n","    base_tokens = [tokenizer.encode(word, add_special_tokens=False) for word in base_words]\n","    target_tokens = [tokenizer.encode(word, add_special_tokens=False) for word in target_words]\n","\n","    return base_tokens, target_tokens\n","\n","# 2. Embedding 계산 및 Conceptual Vector 생성\n","def compute_conceptual_vectors(base_tokens, target_tokens, model, tokenizer, device=\"cuda\"):\n","    base_sentences = [tokenizer.decode(tokens) for tokens in base_tokens]\n","    target_sentences = [tokenizer.decode(tokens) for tokens in target_tokens]\n","\n","    sentences = base_sentences + target_sentences\n","    embeddings = get_embeddings(sentences, model, tokenizer, device)\n","\n","    base_embeddings = embeddings[:len(base_tokens)]\n","    target_embeddings = embeddings[len(base_tokens):]\n","\n","    diff_embeddings = target_embeddings - base_embeddings\n","    mean_vector = torch.mean(diff_embeddings, dim=0)\n","    return mean_vector\n","\n","def get_embeddings(sentences, model, tokenizer, device=\"cuda\", max_length=512):\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    # max_length와 truncation 추가\n","    inputs = tokenizer(\n","        sentences,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=max_length\n","    ).to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","        hidden_states = outputs.hidden_states[-1]\n","\n","    attention_mask = inputs[\"attention_mask\"]\n","    expanded_mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n","    summed_embeddings = torch.sum(hidden_states * expanded_mask, dim=1)\n","    summed_mask = torch.sum(expanded_mask, dim=1)\n","    embeddings = summed_embeddings / summed_mask\n","\n","    return embeddings\n","\n","\n","\n","def compute_projection_matrix(conceptual_vectors, hidden_dim):\n","    \"\"\"\n","    Conceptual Vectors로부터 Projection Matrix를 계산 (QR 기반).\n","\n","    Args:\n","        conceptual_vectors: torch.Tensor, Conceptual Vectors (shape: num_concepts x hidden_dim).\n","        hidden_dim: int, 모델의 hidden dimension (예: 1024).\n","\n","    Returns:\n","        projection_matrix: torch.Tensor, Projection Matrix (shape: hidden_dim x hidden_dim).\n","    \"\"\"\n","    # QR 분해를 사용하여 orthogonal basis 계산\n","    orthogonal_basis = torch.linalg.qr(conceptual_vectors.T)[0]\n","\n","    # Projection Matrix 계산 (hidden_dim 크기로 확장)\n","    projection_matrix = orthogonal_basis @ orthogonal_basis.T\n","    return projection_matrix\n","\n","\n","\n","# 5. Orthogonal Procrustes Analysis (OPA)\n","def orthogonal_procrustes(A, B):\n","    \"\"\"\n","    두 Projection Matrix를 align하기 위한 Orthogonal Procrustes Analysis.\n","    \"\"\"\n","    M = B.T @ A\n","    U, _, V = torch.linalg.svd(M)\n","    R = U @ V.T\n","    return R\n","\n","import os\n","import torch\n","\n","import os\n","import torch\n","\n","def run_experiment_for_all_files(directory, model_1, tokenizer_1, model_2, tokenizer_2, max_iters=100, lr=1e-5):\n","    \"\"\"\n","    Run the experiment for all word pair files in the given directory.\n","\n","    Args:\n","        directory (str): Directory containing word pair files.\n","        model_1 (transformers.PreTrainedModel): First model.\n","        tokenizer_1 (transformers.PreTrainedTokenizer): Tokenizer for the first model.\n","        model_2 (transformers.PreTrainedModel): Second model.\n","        tokenizer_2 (transformers.PreTrainedTokenizer): Tokenizer for the second model.\n","        max_iters (int): Maximum number of iterations for gradient descent.\n","        lr (float): Learning rate for gradient descent.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    all_conceptual_vectors_1 = []\n","    all_conceptual_vectors_2 = []\n","\n","    # Iterate over all files in the directory\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".txt\"):\n","            filepath = os.path.join(directory, filename)\n","            print(f\"Processing file: {filepath}\")\n","\n","            # Step 1: Load counterfactual pairs and compute tokens\n","            base_tokens_1, target_tokens_1 = get_counterfactual_pairs(filepath, tokenizer_1)\n","            base_tokens_2, target_tokens_2 = get_counterfactual_pairs(filepath, tokenizer_2)\n","\n","            # Step 2: Compute conceptual vectors for both models\n","            conceptual_vector_1 = compute_conceptual_vectors(base_tokens_1, target_tokens_1, model_1, tokenizer_1)\n","            conceptual_vector_2 = compute_conceptual_vectors(base_tokens_2, target_tokens_2, model_2, tokenizer_2)\n","\n","            all_conceptual_vectors_1.append(conceptual_vector_1)\n","            all_conceptual_vectors_2.append(conceptual_vector_2)\n","\n","    # Step 3: Compute projection matrices for both models\n","    conceptual_vectors_1 = torch.stack(all_conceptual_vectors_1)\n","    conceptual_vectors_2 = torch.stack(all_conceptual_vectors_2)\n","\n","    hidden_dim = conceptual_vectors_1.size(1)  # Assuming conceptual vectors are of shape (num_concepts, hidden_dim)\n","    P1 = compute_projection_matrix(conceptual_vectors_1, hidden_dim)\n","    P2 = compute_projection_matrix(conceptual_vectors_2, hidden_dim)\n","\n","    print(\"Projection matrices computed.\")\n","\n","    # Step 4: Optimize R using SO(n) gradient descent\n","    R = optimize_R_on_SO(P1, P2, max_iters=max_iters, lr=lr)\n","\n","    # Step 5: Update model_2's embedding layer with R\n","    update_model_with_R(model_2, R)\n","\n","    print(\"Model 2 updated with optimized R.\")\n","\n","# Supporting functions\n","\n","def compute_gradient_so(P1, P2, R):\n","    \"\"\"\n","    Compute the gradient of the loss function J(R) and project it onto so(n).\n","\n","    Args:\n","        P1 (torch.Tensor): Projection matrix of model 1 (shape: d x d).\n","        P2 (torch.Tensor): Projection matrix of model 2 (shape: d x d).\n","        R (torch.Tensor): Current rotation matrix (shape: d x d).\n","\n","    Returns:\n","        torch.Tensor: Gradient projected onto so(n) (shape: d x d).\n","    \"\"\"\n","\n","    max_norm=1.0\n","    # Compute the loss gradient before projection\n","    diff = P1 - R @ P2 @ R.T\n","    grad = -4 * (diff @ P2 @ R)\n","\n","    grad_norm = torch.norm(grad)\n","    if grad_norm > max_norm:\n","        grad = grad * (max_norm / grad_norm)\n","\n","\n","    # Project the gradient onto so(n)\n","    grad_antisym = 0.5 * (grad - grad.T)\n","    return grad_antisym\n","\n","def optimize_R_on_SO(P1, P2, max_iters=100, lr=1e-2):\n","    \"\"\"\n","    Perform optimization on SO(n) to find the optimal R.\n","\n","    Args:\n","        P1 (torch.Tensor): Projection matrix of model 1 (shape: d x d).\n","        P2 (torch.Tensor): Projection matrix of model 2 (shape: d x d).\n","        max_iters (int): Maximum number of iterations.\n","        lr (float): Learning rate.\n","\n","    Returns:\n","        torch.Tensor: Optimized rotation matrix R (shape: d x d).\n","    \"\"\"\n","    # Initialize R as the identity matrix\n","    #d = P1.shape[0]\n","    #R = torch.eye(d, device=P1.device)\n","\n","    #U, _, V = torch.svd(P1 @ P2.T)\n","    #R = (U @ V.T).clone()  # 초기화된 R\n","    device=\"cuda\"\n","    hidden_dim=1024\n","    R = torch.eye(hidden_dim).to(device) + 0.01 * torch.randn(hidden_dim, hidden_dim).to(device)\n","    R = torch.linalg.qr(R)[0]\n","\n","\n","    for i in range(max_iters):\n","        # Compute the gradient projected onto so(n)\n","        grad_so = compute_gradient_so(P1, P2, R)\n","\n","        # Gradient descent step\n","        R = R - lr * grad_so\n","\n","        # Re-project R onto SO(n) using SVD\n","        U, _, V = torch.svd(R)\n","        R = U @ V.T\n","\n","        # Compute the loss\n","        diff = P1 - R @ P2 @ R.T\n","        loss = torch.trace(diff @ diff.T)\n","\n","        # Print progress\n","        print(f\"Iteration {i+1}/{max_iters}, Loss: {loss.item()}\")\n","\n","    return R\n","\n","def compute_projection_matrix(conceptual_vectors, hidden_dim):\n","    \"\"\"\n","    Compute the projection matrix for a set of conceptual vectors.\n","\n","    Args:\n","        conceptual_vectors (torch.Tensor): Conceptual vectors (shape: num_concepts x hidden_dim).\n","        hidden_dim (int): Dimensionality of the hidden space.\n","\n","    Returns:\n","        torch.Tensor: Projection matrix (shape: hidden_dim x hidden_dim).\n","    \"\"\"\n","    orthogonal_basis = torch.linalg.qr(conceptual_vectors.T)[0]\n","    projection_matrix = orthogonal_basis @ orthogonal_basis.T\n","    return projection_matrix\n"],"metadata":{"id":"tauNC2lOcpuo","executionInfo":{"status":"ok","timestamp":1733919254146,"user_tz":-540,"elapsed":438,"user":{"displayName":"구준모","userId":"09347892568623850386"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# 모델 및 토크나이저 설정\n","model_1_name = \"gpt2-medium\"\n","model_2_name = \"bigscience/bloom-560m\"\n","\n","tokenizer_1 = AutoTokenizer.from_pretrained(model_1_name)\n","model_1 = AutoModelForCausalLM.from_pretrained(model_1_name).to(\"cuda\")\n","\n","tokenizer_2 = AutoTokenizer.from_pretrained(model_2_name)\n","model_2 = AutoModelForCausalLM.from_pretrained(model_2_name).to(\"cuda\")\n","\n","# word_pairs 디렉토리에서 모든 파일 실행\n","directory = \"word_pairs\"\n","run_experiment_for_all_files(directory, model_1, tokenizer_1, model_2, tokenizer_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9hNVnc8ec-P","executionInfo":{"status":"ok","timestamp":1733919276024,"user_tz":-540,"elapsed":20833,"user":{"displayName":"구준모","userId":"09347892568623850386"}},"outputId":"326aa99f-f77c-4cdb-a2d0-46af5cdef07e"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file: word_pairs/[adj - comparative].txt\n","Processing file: word_pairs/[Ving - Ved].txt\n","Processing file: word_pairs/[French - German].txt\n","Processing file: word_pairs/[3pSg - Ved].txt\n","Processing file: word_pairs/[adj - superlative].txt\n","Processing file: word_pairs/[adj - adj + ly].txt\n","Processing file: word_pairs/[Ving - 3pSg].txt\n","Processing file: word_pairs/[English - French].txt\n","Processing file: word_pairs/[French - Spanish].txt\n","Processing file: word_pairs/[German - Spanish].txt\n","Processing file: word_pairs/[frequent - infrequent].txt\n","Processing file: word_pairs/[verb - V + ment].txt\n","Processing file: word_pairs/[adj - un + adj].txt\n","Processing file: word_pairs/[small - big].txt\n","Processing file: word_pairs/[thing - part].txt\n","Processing file: word_pairs/[verb - V + er].txt\n","Processing file: word_pairs/[verb - 3pSg].txt\n","Processing file: word_pairs/[verb - V + able].txt\n","Processing file: word_pairs/[verb - Ving].txt\n","Processing file: word_pairs/[verb - V + tion].txt\n","Processing file: word_pairs/[verb - Ved].txt\n","Processing file: word_pairs/[thing - color].txt\n","Processing file: word_pairs/[lower - upper].txt\n","Processing file: word_pairs/[pronoun - possessive].txt\n","Processing file: word_pairs/[noun - plural].txt\n","Processing file: word_pairs/[country - capital].txt\n","Processing file: word_pairs/[male - female].txt\n","Projection matrices computed.\n","Iteration 1/100, Loss: 52.867332458496094\n","Iteration 2/100, Loss: 52.868953704833984\n","Iteration 3/100, Loss: 52.86903381347656\n","Iteration 4/100, Loss: 52.868927001953125\n","Iteration 5/100, Loss: 52.86925506591797\n","Iteration 6/100, Loss: 52.86910629272461\n","Iteration 7/100, Loss: 52.86903762817383\n","Iteration 8/100, Loss: 52.86939239501953\n","Iteration 9/100, Loss: 52.86900329589844\n","Iteration 10/100, Loss: 52.87019348144531\n","Iteration 11/100, Loss: 52.86924743652344\n","Iteration 12/100, Loss: 52.868408203125\n","Iteration 13/100, Loss: 52.86866760253906\n","Iteration 14/100, Loss: 52.86967468261719\n","Iteration 15/100, Loss: 52.86956787109375\n","Iteration 16/100, Loss: 52.86919403076172\n","Iteration 17/100, Loss: 52.86859893798828\n","Iteration 18/100, Loss: 52.86915588378906\n","Iteration 19/100, Loss: 52.868927001953125\n","Iteration 20/100, Loss: 52.869293212890625\n","Iteration 21/100, Loss: 52.86870574951172\n","Iteration 22/100, Loss: 52.868919372558594\n","Iteration 23/100, Loss: 52.869510650634766\n","Iteration 24/100, Loss: 52.869476318359375\n","Iteration 25/100, Loss: 52.86908721923828\n","Iteration 26/100, Loss: 52.86921310424805\n","Iteration 27/100, Loss: 52.8693733215332\n","Iteration 28/100, Loss: 52.868743896484375\n","Iteration 29/100, Loss: 52.86874008178711\n","Iteration 30/100, Loss: 52.868751525878906\n","Iteration 31/100, Loss: 52.86919403076172\n","Iteration 32/100, Loss: 52.86915588378906\n","Iteration 33/100, Loss: 52.869163513183594\n","Iteration 34/100, Loss: 52.86897659301758\n","Iteration 35/100, Loss: 52.86859893798828\n","Iteration 36/100, Loss: 52.86933135986328\n","Iteration 37/100, Loss: 52.86943435668945\n","Iteration 38/100, Loss: 52.86876678466797\n","Iteration 39/100, Loss: 52.869171142578125\n","Iteration 40/100, Loss: 52.86945343017578\n","Iteration 41/100, Loss: 52.869529724121094\n","Iteration 42/100, Loss: 52.86907958984375\n","Iteration 43/100, Loss: 52.868896484375\n","Iteration 44/100, Loss: 52.869171142578125\n","Iteration 45/100, Loss: 52.86806869506836\n","Iteration 46/100, Loss: 52.86821746826172\n","Iteration 47/100, Loss: 52.869384765625\n","Iteration 48/100, Loss: 52.869266510009766\n","Iteration 49/100, Loss: 52.86777877807617\n","Iteration 50/100, Loss: 52.86888122558594\n","Iteration 51/100, Loss: 52.86884307861328\n","Iteration 52/100, Loss: 52.86962890625\n","Iteration 53/100, Loss: 52.869163513183594\n","Iteration 54/100, Loss: 52.86905288696289\n","Iteration 55/100, Loss: 52.868614196777344\n","Iteration 56/100, Loss: 52.86890411376953\n","Iteration 57/100, Loss: 52.868743896484375\n","Iteration 58/100, Loss: 52.868797302246094\n","Iteration 59/100, Loss: 52.868621826171875\n","Iteration 60/100, Loss: 52.86946105957031\n","Iteration 61/100, Loss: 52.86894989013672\n","Iteration 62/100, Loss: 52.8690071105957\n","Iteration 63/100, Loss: 52.86918640136719\n","Iteration 64/100, Loss: 52.86838912963867\n","Iteration 65/100, Loss: 52.869110107421875\n","Iteration 66/100, Loss: 52.86967849731445\n","Iteration 67/100, Loss: 52.869049072265625\n","Iteration 68/100, Loss: 52.868465423583984\n","Iteration 69/100, Loss: 52.8685302734375\n","Iteration 70/100, Loss: 52.86909484863281\n","Iteration 71/100, Loss: 52.869197845458984\n","Iteration 72/100, Loss: 52.868934631347656\n","Iteration 73/100, Loss: 52.86850357055664\n","Iteration 74/100, Loss: 52.86942672729492\n","Iteration 75/100, Loss: 52.867958068847656\n","Iteration 76/100, Loss: 52.86933135986328\n","Iteration 77/100, Loss: 52.868736267089844\n","Iteration 78/100, Loss: 52.86904525756836\n","Iteration 79/100, Loss: 52.86917495727539\n","Iteration 80/100, Loss: 52.86895751953125\n","Iteration 81/100, Loss: 52.86933898925781\n","Iteration 82/100, Loss: 52.86919403076172\n","Iteration 83/100, Loss: 52.8682861328125\n","Iteration 84/100, Loss: 52.86909484863281\n","Iteration 85/100, Loss: 52.86884307861328\n","Iteration 86/100, Loss: 52.868743896484375\n","Iteration 87/100, Loss: 52.86838912963867\n","Iteration 88/100, Loss: 52.86857223510742\n","Iteration 89/100, Loss: 52.86819839477539\n","Iteration 90/100, Loss: 52.868980407714844\n","Iteration 91/100, Loss: 52.86947250366211\n","Iteration 92/100, Loss: 52.86915588378906\n","Iteration 93/100, Loss: 52.86866760253906\n","Iteration 94/100, Loss: 52.86946487426758\n","Iteration 95/100, Loss: 52.8690299987793\n","Iteration 96/100, Loss: 52.86919021606445\n","Iteration 97/100, Loss: 52.868797302246094\n","Iteration 98/100, Loss: 52.86936569213867\n","Iteration 99/100, Loss: 52.86823272705078\n","Iteration 100/100, Loss: 52.86939239501953\n","Model embedding layer updated with R.\n","Model 2 updated with optimized R.\n"]}]},{"cell_type":"code","source":["def update_model_with_R(model, R):\n","    \"\"\"\n","    모델의 embedding layer를 정렬 행렬 R을 사용해 업데이트.\n","\n","    Args:\n","        model: Pretrained 모델.\n","        R: torch.Tensor, 학습된 정렬 행렬 (shape: hidden_dim x hidden_dim).\n","    \"\"\"\n","    # 모델의 embedding layer 가져오기\n","    embedding_layer = model.get_input_embeddings()\n","\n","    # 기존 embedding layer의 가중치\n","    embedding_weights = embedding_layer.weight.data\n","\n","    # 정렬 행렬 R을 이용하여 embedding 가중치 변환\n","    transformed_weights = embedding_weights @ R.T\n","\n","    # 업데이트된 embedding 가중치를 embedding layer에 반영\n","    embedding_layer.weight.data = transformed_weights\n","\n","    print(\"Model embedding layer updated with R.\")\n"],"metadata":{"id":"oELdtwIIesuT","executionInfo":{"status":"aborted","timestamp":1733919219118,"user_tz":-540,"elapsed":4,"user":{"displayName":"구준모","userId":"09347892568623850386"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SrSt6w6Ae8gp"},"execution_count":null,"outputs":[]}]}